{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2BW9eMAVAKj",
        "outputId": "572aea13-2250-4504-8fa9-163103f89a82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_validate\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import LeaveOneOut, GridSearchCV\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.inspection import permutation_importance\n",
        "from scipy.stats import ttest_ind\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xeZkH3LVCYo"
      },
      "outputs": [],
      "source": [
        "def apply_global_signal_regression(timeseries: np.ndarray) -> np.ndarray:\n",
        "    global_signal = np.mean(timeseries, axis=1, keepdims=True)\n",
        "    return timeseries - global_signal\n",
        "\n",
        "def build_correlation_matrix(timeseries: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Compute correlation matrix while handling NaNs and checking constant regions.\"\"\"\n",
        "    timeseries = np.nan_to_num(timeseries, nan=0.0)\n",
        "    variances = np.var(timeseries, axis=0)\n",
        "    zero_var_regions = np.where(variances == 0)[0]\n",
        "    if len(zero_var_regions) > 0:\n",
        "        return None\n",
        "    corr_matrix = np.corrcoef(timeseries, rowvar=False)\n",
        "    return np.clip(np.nan_to_num(corr_matrix, nan=0.0, posinf=1.0, neginf=-1.0), -1.0, 1.0)\n",
        "\n",
        "def flatten_upper_triangle(corr_matrix: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Extract the upper triangle (excluding diagonal) as a feature vector.\"\"\"\n",
        "    triu_indices = np.triu_indices(corr_matrix.shape[0], k=1)\n",
        "    return corr_matrix[triu_indices]\n",
        "\n",
        "def load_timeseries_data(ad_dir, cn_dir):\n",
        "    ad_files = sorted(glob.glob(os.path.join(ad_dir, \"*.txt\")))\n",
        "    cn_files = sorted(glob.glob(os.path.join(cn_dir, \"*.txt\")))\n",
        "\n",
        "    X, y = [], []\n",
        "\n",
        "    for fpath in ad_files:\n",
        "        data_array = np.loadtxt(fpath)\n",
        "        data_array = apply_global_signal_regression(data_array)\n",
        "        corr_mat = build_correlation_matrix(data_array)\n",
        "\n",
        "        if corr_mat is not None:\n",
        "            X.append(flatten_upper_triangle(corr_mat))\n",
        "            y.append(1)\n",
        "\n",
        "    for fpath in cn_files:\n",
        "        data_array = np.loadtxt(fpath)\n",
        "        corr_mat = build_correlation_matrix(data_array)\n",
        "        if corr_mat is not None:\n",
        "            X.append(flatten_upper_triangle(corr_mat))\n",
        "            y.append(0)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def evaluate_model_with_cv(X, y, n_splits=5):\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    svm = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\")\n",
        "\n",
        "    scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    scores = cross_validate(svm, X_scaled, y, cv=skf, scoring=scoring, return_train_score=False)\n",
        "\n",
        "    results = {metric: (np.mean(scores[f'test_{metric}']), np.std(scores[f'test_{metric}']))\n",
        "               for metric in scoring}\n",
        "\n",
        "    sensitivities = []\n",
        "    specificities = []\n",
        "\n",
        "    for train_idx, test_idx in skf.split(X_scaled, y):\n",
        "        svm.fit(X_scaled[train_idx], y[train_idx])\n",
        "        y_pred = svm.predict(X_scaled[test_idx])\n",
        "        cm = confusion_matrix(y[test_idx], y_pred)\n",
        "        tn, fp, fn, tp = cm.ravel() if cm.shape == (2, 2) else (0, 0, 0, 0)\n",
        "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        sensitivities.append(sensitivity)\n",
        "        specificities.append(specificity)\n",
        "\n",
        "    results[\"Sensitivity\"] = (np.mean(sensitivities), np.std(sensitivities))\n",
        "    results[\"Specificity\"] = (np.mean(specificities), np.std(specificities))\n",
        "\n",
        "    return results\n",
        "\n",
        "def evaluate_with_loocv(X, y):\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    svm = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\")\n",
        "    loo = LeaveOneOut()\n",
        "\n",
        "    y_true, y_pred = [], []\n",
        "    for train_idx, test_idx in loo.split(X_scaled, y):\n",
        "        svm.fit(X_scaled[train_idx], y[train_idx])\n",
        "        pred = svm.predict(X_scaled[test_idx])\n",
        "        y_true.append(y[test_idx][0])\n",
        "        y_pred.append(pred[0])\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel() if cm.shape == (2, 2) else (0, 0, 0, 0)\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "    results = {\n",
        "        \"accuracy\": (accuracy_score(y_true, y_pred), 0.0),\n",
        "        \"precision\": (precision_score(y_true, y_pred, zero_division=0), 0.0),\n",
        "        \"recall\": (recall_score(y_true, y_pred, zero_division=0), 0.0),\n",
        "        \"f1\": (f1_score(y_true, y_pred, zero_division=0), 0.0),\n",
        "        \"Sensitivity\": (sensitivity, 0.0),\n",
        "        \"Specificity\": (specificity, 0.0)\n",
        "    }\n",
        "    return results\n",
        "\n",
        "def evaluate_with_nested_cv(X, y, outer_folds=5, inner_folds=3):\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1.0, 10.0],\n",
        "        'gamma': ['scale', 0.1, 0.01]\n",
        "    }\n",
        "\n",
        "    outer_cv = StratifiedKFold(n_splits=outer_folds, shuffle=True, random_state=42)\n",
        "    inner_cv = StratifiedKFold(n_splits=inner_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    accuracies, precisions, recalls, f1s, sensitivities, specificities = [], [], [], [], [], []\n",
        "\n",
        "    for train_idx, test_idx in outer_cv.split(X_scaled, y):\n",
        "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        grid = GridSearchCV(SVC(kernel=\"rbf\"), param_grid, cv=inner_cv, scoring='accuracy')\n",
        "        grid.fit(X_train, y_train)\n",
        "\n",
        "        best_model = grid.best_estimator_\n",
        "        y_pred = best_model.predict(X_test)\n",
        "\n",
        "        accuracies.append(accuracy_score(y_test, y_pred))\n",
        "        precisions.append(precision_score(y_test, y_pred, zero_division=0))\n",
        "        recalls.append(recall_score(y_test, y_pred, zero_division=0))\n",
        "        f1s.append(f1_score(y_test, y_pred, zero_division=0))\n",
        "\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        tn, fp, fn, tp = cm.ravel() if cm.shape == (2, 2) else (0, 0, 0, 0)\n",
        "        sens = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        spec = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        sensitivities.append(sens)\n",
        "        specificities.append(spec)\n",
        "\n",
        "    results = {\n",
        "        \"accuracy\": (np.mean(accuracies), np.std(accuracies)),\n",
        "        \"precision\": (np.mean(precisions), np.std(precisions)),\n",
        "        \"recall\": (np.mean(recalls), np.std(recalls)),\n",
        "        \"f1\": (np.mean(f1s), np.std(f1s)),\n",
        "        \"Sensitivity\": (np.mean(sensitivities), np.std(sensitivities)),\n",
        "        \"Specificity\": (np.mean(specificities), np.std(specificities))\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "def print_formatted_results(results_dict):\n",
        "    print(\"\\nFinal SVM Classification Results:\")\n",
        "    print(\"------------------------------------------------------\")\n",
        "    print(f\"{'Metric':<20} {'Mean':<10} {'Std Dev'}\")\n",
        "    print(\"------------------------------------------------------\")\n",
        "    rename = {\n",
        "        \"accuracy\": \"Accuracy\",\n",
        "        \"precision\": \"Precision (AD)\",\n",
        "        \"recall\": \"Recall (AD)\",\n",
        "        \"f1\": \"F1-score (AD)\",\n",
        "        \"Sensitivity\": \"Sensitivity\",\n",
        "        \"Specificity\": \"Specificity\"\n",
        "    }\n",
        "    for metric, (mean, std) in results_dict.items():\n",
        "        label = rename.get(metric, metric)\n",
        "        print(f\"{label:<20} {mean:.2f} ± {std:.2f}\")\n",
        "    print(\"------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt1Yu9IiVJFh",
        "outputId": "50cedfd7-a0ce-45fd-8bc1-cafdc93181f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final SVM Classification Results:\n",
            "------------------------------------------------------\n",
            "Metric               Mean       Std Dev\n",
            "------------------------------------------------------\n",
            "Accuracy             0.96 ± 0.08\n",
            "Precision (AD)       0.94 ± 0.11\n",
            "Recall (AD)          1.00 ± 0.00\n",
            "F1-score (AD)        0.97 ± 0.07\n",
            "Sensitivity          1.00 ± 0.00\n",
            "Specificity          0.92 ± 0.16\n",
            "------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "## AAL90\n",
        "AD_DIR = \"/content/drive/MyDrive/FMRI/Exp1/Timeseries_AAL90/AD\"\n",
        "CN_DIR = \"/content/drive/MyDrive/FMRI/Exp1/Timeseries_AAL90/CN\"\n",
        "\n",
        "X, y = load_timeseries_data(AD_DIR, CN_DIR)\n",
        "results = evaluate_model_with_cv(X, y, n_splits=5)\n",
        "print_formatted_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5zkIOpuVdqA",
        "outputId": "66ee0a57-e78e-446f-89f1-159ba7a6f592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SVM Weight Rank  Permutation Rank  T-test Rank\n",
            "0             1173                 0         2458\n",
            "1             1218              4004         2499\n",
            "2              484              4003         1604\n",
            "3             1172              4002         2414\n",
            "4             1651              4001         1602\n",
            "5             2935                16         2402\n",
            "6              732                17         2567\n",
            "7             1210                18         3742\n",
            "8             1174                19         2358\n",
            "9             1159                20         3764\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "svm_linear = SVC(kernel=\"linear\", C=1.0)\n",
        "svm_linear.fit(X_scaled, y)\n",
        "\n",
        "svm_weights = np.abs(svm_linear.coef_[0])\n",
        "top_idx_svm = np.argsort(svm_weights)[-10:][::-1]\n",
        "\n",
        "perm = permutation_importance(svm_linear, X_scaled, y, n_repeats=30, random_state=42)\n",
        "perm_importance = perm.importances_mean\n",
        "top_idx_perm = np.argsort(perm_importance)[-10:][::-1]\n",
        "\n",
        "X_ad = X[np.array(y) == 1]\n",
        "X_cn = X[np.array(y) == 0]\n",
        "t_vals, p_vals = ttest_ind(X_ad, X_cn, axis=0)\n",
        "top_idx_ttest = np.argsort(np.abs(t_vals))[-10:][::-1]\n",
        "\n",
        "summary_df = pd.DataFrame({\n",
        "    \"SVM Weight Rank\": top_idx_svm,\n",
        "    \"Permutation Rank\": top_idx_perm,\n",
        "    \"T-test Rank\": top_idx_ttest\n",
        "})\n",
        "print(summary_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9ty4oB3VkIt",
        "outputId": "71251038-7e55-4b67-bab2-e73bb7fbaa15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank 1: Feature 1173 → Region 14 ↔ Region 33\n",
            "Rank 2: Feature 1218 → Region 14 ↔ Region 78\n",
            "Rank 3: Feature 484 → Region 5 ↔ Region 55\n",
            "Rank 4: Feature 1172 → Region 14 ↔ Region 32\n",
            "Rank 5: Feature 1651 → Region 20 ↔ Region 82\n",
            "Rank 6: Feature 2935 → Region 43 ↔ Region 55\n",
            "Rank 7: Feature 732 → Region 8 ↔ Region 57\n",
            "Rank 8: Feature 1210 → Region 14 ↔ Region 70\n",
            "Rank 9: Feature 1174 → Region 14 ↔ Region 34\n",
            "Rank 10: Feature 1159 → Region 14 ↔ Region 19\n"
          ]
        }
      ],
      "source": [
        "def get_upper_triangle_pairs(n_regions=90):\n",
        "    triu_idx = np.triu_indices(n_regions, k=1)\n",
        "    return list(zip(triu_idx[0], triu_idx[1]))\n",
        "\n",
        "region_pairs = get_upper_triangle_pairs()\n",
        "\n",
        "for rank, idx in enumerate(top_idx_svm):\n",
        "    i, j = region_pairs[idx]\n",
        "    print(f\"Rank {rank+1}: Feature {idx} → Region {i} ↔ Region {j}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
